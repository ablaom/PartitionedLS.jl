var documenterSearchIndex = {"docs":
[{"location":"#Partitioned-Least-Squares","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"","category":"section"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"Linear least squares is one of the most widely used regression methods among scientists in many fields. The simplicity of the model allows this method to be used when data is scarce and it is usually appealing to practitioners that need to gather some insight into the problem by inspecting the values of the learnt parameters. PartitionedLS is a variant of the linear least squares model allowing practitioners to partition the input features into groups of variables that they require to contribute similarly to the final result. ","category":"page"},{"location":"#The-model","page":"Partitioned Least Squares","title":"The model","text":"","category":"section"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"The Partitioned Least Squares model is formally defined as:","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"begingather*\ntextminimize_mathbfalpha mathbfbeta  mathbfX times (mathbfP circ mathbfalpha) times mathbfbeta - mathbfy _2^2 \nbeginaligned\nquad stquad  mathbfalpha  succeq 0\n                    mathbfP^T times mathbfalpha = mathbf1\nendaligned\nendgather*","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"where: ","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"mathbfX is N times M data matrix;\nmathbfP is a user-defined partition matrix having K columns (one for each element of the partition), M rows, and containing 1 in P_ij if the i-th attribute belongs to the j-th partition and 0 otherwise;\nmathbfbeta is a vector weighting the importance of each set of attributes in the partition;\nmathbfalpha is a vector weighting the importance of each attribute within one of the sets in the partition. Note that the constraints imply that for each set in the partition the weights of the corresponding alpha variables are all positive and sum to 1.","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"The PartitionedLS problem is non-convex and NP-complete. The library provides two algorithms to solve the problem anyway: an iterative algorithm based on the Alternating Least Squares approach and an optimal algorithm that guarantees requiring however exponential time in the cardinality of the partition (i.e., it is mainly useful when K is small).","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"More details can be found in the paper Partitioned Least Squares.","category":"page"},{"location":"#To-install-this-library","page":"Partitioned Least Squares","title":"To install this library","text":"","category":"section"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"Just add it as a dependency to your Julia environment. Launch julia from the main directory of your project and enter the following commands:","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"# Opens the package manager REPL\n]\n\n# Activate you local environment (can be skipped if you want to install the library globally)\nactivate .\n\n# Adds the library to the environment\nadd PartitionedLS","category":"page"},{"location":"#To-use-this-library","page":"Partitioned Least Squares","title":"To use this library","text":"","category":"section"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"You will need a matrix P describing the partitioning of your variables, e.g.:","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"P = [[1 0]; \n     [1 0]; \n     [0 1]]","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"specifies that the first and the second variable belongs to the first partition, while the third variable belongs to the second.","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"You then just give your data to the fit function and use the predict function to make predictions. ","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"A complete example:","category":"page"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"using PartitionedLS\n\nX = [[1. 2. 3.]; \n     [3. 3. 4.]; \n     [8. 1. 3.]; \n     [5. 3. 1.]]\n\ny = [1.; \n     1.; \n     2.; \n     3.]\n\nP = [[1 0]; \n     [1 0]; \n     [0 1]]\n\n\n# fit using the optimal algorithm \nresult_opt = fit(Opt, X, y, P, η = 0.0)\n\n# fit using the iterative algorithm\nresult_alt = fit(Alt, X, y, P, η = 0.0)\n\n# fit using the BnB algorithm\nresult_alt_nnls = fit(BnB, X, y, P)\n\n# Make predictions on the given data matrix. The function works\n# with results returned by anyone of the solvers.\npredict(result_opt.model, X)","category":"page"},{"location":"#Function-Documentation","page":"Partitioned Least Squares","title":"Function Documentation","text":"","category":"section"},{"location":"","page":"Partitioned Least Squares","title":"Partitioned Least Squares","text":"fit\npredict","category":"page"},{"location":"#PartitionedLS.fit","page":"Partitioned Least Squares","title":"PartitionedLS.fit","text":"fit(::Type{Alt}, X::Matrix{Float64}, y::Vector{Float64}, P::Matrix{Int}; η = 0.0, ϵ = 1e-6, T = 100, nnlsalg = :nnls)\n\nFits a PartitionedLS model by alternating the optimization of the α and β variables. This version uses  an optimization strategy based on non-negative-least-squaes solvers. This formulation is faster and  more numerically stable with respect to fit(Alt, ...)`.\n\nArguments\n\nX: N  M matrix describing the examples\ny: N vector with the output values for each example\nP: M  K matrix specifying how to partition the M attributes into K subsets. P_mk should be 1 if attribute number m belongs to\n\npartition k.\n\nη: regularization factor, higher values implies more regularized solutions\nT: number of alternating loops to be performed, defaults to 1000.\nϵ: minimum relative improvement in the objective function before stopping the optimization. Default is 1e-6\nnnlsalg: specific flavour of nnls algorithm to be used, possible values are :pivot, :nnls, :fnnls, default is :nnls\n\nResult\n\nA NamedTuple with the following fields:\n\nopt: optimal value of the objective function (loss + regularization)\nmodel: a NamedTuple containing the following fields:\na: values of the α variables at the optimal point\nb: values of the β variables at the optimal point\nt: the intercept at the optimal point\nP: the partition matrix (copied from the input)\n\n\n\n\n\nfit(::Type{Opt}, X::Matrix{Float64}, y::Vector{Float64}, P::Matrix{Int}; η=0.0, returnAllSolutions=false, nnlsalg=:nnls)\n\nFits a PartialLS Regression model to the given data and resturns the learnt model (see the Result section).  It uses a coplete enumeration strategy which is exponential in K, but guarantees to find the optimal solution.\n\nArguments\n\nX: N  M matrix describing the examples\ny: N vector with the output values for each example\nP: M  K matrix specifying how to partition the M attributes into K subsets. P_mk should be 1 if attribute number m belongs to\n\npartition k.\n\nη: regularization factor, higher values implies more regularized solutions (default: 0.0)\nreturnAllSolutions: if true an additional output is appended to the resulting tuple containing all solutions found during the algorithm.\nnnlsalg: the kind of nnls algorithm to be used during solving. Possible values are :pivot, :nnls, :fnnls (default: :nnls)\n\nResult\n\nA NamedTuple with the following fields:\n\nopt: optimal value of the objective function (loss + regularization)\nmodel: a NamedTuple containing the following fields:\na: values of the α variables at the optimal point\nb: values of the β variables at the optimal point\nt: the intercept at the optimal point\nP: the partition matrix (copied from the input)\nsolutions: all solutions found during the execution (returned only if resultAllSolutions=true)\n\nExample\n\nX = rand(100, 10)\ny = rand(100)\nP = [1 0 0; 0 1 0; 0 0 1; 1 1 0; 0 1 1]\nresult = fit(Opt, X, y, P)\n\n\n\n\n\nfit(::Type{BnB}, X::Matrix{Float64}, y::Vector{Float64}, P::Matrix{Int}, η=0.0, nnlsalg=:nnls)\n\nImplements the Branch and Bound algorithm to fit a Partitioned Least Squres model.\n\nArguments\n\nX: N  M matrix describing the examples\ny: N vector with the output values for each example\nP: M  K matrix specifying how to partition the M attributes into K subsets. P_mk should be 1 if attribute number m belongs to\n\npartition k.\n\nη: regularization factor, higher values implies more regularized solutions (default: 0.0)\nnnlsalg: the kind of nnls algorithm to be used during solving. Possible values are :pivot, :nnls, :fnnls (default: :nnls)\n\nResult\n\nA tuple of the form: (opt, a, b, t, P, nopen)\n\nopt: optimal value of the objective function (loss + regularization)\nmodel: a NamedTuple containing the following fields:\na: values of the α variables at the optimal point\nb: values of the β variables at the optimal point\nt: the intercept at the optimal point\nP: the partition matrix (copied from the input)\nnopen: the number of nodes opened by the BnB algorithm\n\n\n\n\n\n","category":"function"},{"location":"#PartitionedLS.predict","page":"Partitioned Least Squares","title":"PartitionedLS.predict","text":"predict(α::Vector{Float64}, β::Vector{Float64}, t::Float64, P::Matrix{Int}, X::Matrix{Float64})::Vector{Float64}\n\nResult\n\nthe prediction for the partitioned least squares problem with solution α, β, t over the dataset X and partition matrix P\n\n\n\n\n\npredict(model, X)::Vector{Float64}\n\nMake predictions for the datataset X using the PartialLS model model.\n\nArguments\n\nmodel is a NamedTuple in the form returned by fit functions, it shall contains the following elements:\nopt: the optimal value of the objective attained by the fit function\n'model': a NamedTuple containing the following elements:\nα: the values of the α variables\nβ: the values of the β variables\nt: the value of the t variable\nP: the partition matrix\nX: the data containing the examples for which the predictions are sought\n\nReturn\n\nthe predictions of the given model on examples in X. \n\n\n\n\n\n","category":"function"}]
}
